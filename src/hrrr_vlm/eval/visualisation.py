"""Module for clustering embeddings generated by fine-tuned SigLIP models to
discover patterns.
"""

from collections import Counter
from typing import Any, Literal, NamedTuple

import numpy as np
import umap
from scipy.cluster.hierarchy import fcluster, linkage
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import calinski_harabasz_score, silhouette_score
from sklearn.preprocessing import StandardScaler

from hrrr_vlm.eval.embeddings import EmbeddingData
from hrrr_vlm.utils.logger import configure_logger

# Configure logging
logger = configure_logger()


class ClusteringResults(NamedTuple):
    """Results from clustering analysis.

    Attributes:
        labels (`np.ndarray`): Cluster labels for each sample.
        n_clusters (`int`): Number of clusters found.
        silhouette_score (`float`): Silhouette coefficient for cluster quality.
        calinski_harabasz_score (`float`): Calinski-Harabasz index for cluster
            quality.
        reduced_embeddings (`np.ndarray`): 2D/3D reduced embeddings for visualisation.
        cluster_centers (`np.ndarray`): Centers of clusters (if applicable).
        metadata_analysis (`dict[str, Any]`): Analysis of how clusters align with
            metadata.
    """

    labels: np.ndarray
    n_clusters: int
    silhouette_score: float
    calinski_harabasz_score: float
    reduced_embeddings: np.ndarray
    cluster_centers: np.ndarray | None
    metadata_analysis: dict[str, Any]


class EmbeddingClusterer:
    """Clustering analyser for SigLIP generated embeddings with metadata integration."""

    def __init__(
        self,
        *,
        random_state: int = 42,
        reduction_method: Literal["pca", "tsne", "umap"] = "umap",
        n_components: int = 3,
    ) -> None:
        """Initialise the embedding clusterer.

        Args:
            random_state (`int`): Random seed for reproducibility.
            reduction_method (`str`): Method for dimensionality reduction ('umap',
                'tsne', 'pca').
            n_components (`int`): Number of components for dimensionality reduction.
        """
        self.random_state = random_state
        self.n_components = n_components
        self.reduction_method = reduction_method
        self._setup_reducer()

    def _setup_reducer(self) -> None:
        """Set up the dimensionality reduction method.

        Raises:
            ValueError: If an unsupported reduction method is specified.
        """
        if self.reduction_method == "umap":
            self.reducer = umap.UMAP(
                n_components=self.n_components,
                random_state=self.random_state,
                n_neighbors=15,
                min_dist=0.1,
                metric="cosine",
            )
        elif self.reduction_method == "tsne":
            self.reducer = TSNE(
                n_components=self.n_components,
                random_state=self.random_state,
                perplexity=30,
                n_iter=1000,
                metric="cosine",
                init="pca",
            )
        elif self.reduction_method == "pca":
            self.reducer = PCA(
                n_components=self.n_components, random_state=self.random_state
            )
        else:
            msg = f"Unsupported reduction method: {self.reduction_method}"
            raise ValueError(msg)

    def cluster_kmeans(
        self,
        embedding_data: EmbeddingData,
        n_clusters: int = 8,
        *,
        embedding_type: Literal["image", "text", "combined"] = "combined",
        standardise: bool = True,
    ) -> ClusteringResults:
        """Perform K-Means clustering on weather embeddings.

        Args:
            embedding_data (`EmbeddingData`): Embedding data to cluster.
            n_clusters (`int`): Number of clusters to create.
            embedding_type (`str`): Type of embeddings to use ('image', 'text',
                'combined').
            standardise (`bool`): Whether to standardize embeddings before clustering.

        Returns:
            `ClusteringResults`: Container for clustering analysis.
        """
        embeddings = self.prepare_embeddings(embedding_data, embedding_type)

        if standardise:
            scaler = StandardScaler()
            embeddings = scaler.fit_transform(embeddings)

        # Perform K-Means clustering
        kmeans = KMeans(
            n_clusters=n_clusters,
            random_state=self.random_state,
            n_init=10,
            max_iter=300,
        )
        labels = kmeans.fit_predict(embeddings)

        return self.create_clustering_results(
            embeddings=embeddings,
            labels=labels,
            cluster_centers=kmeans.cluster_centers_,
            embedding_data=embedding_data,
        )

    def cluster_dbscan(
        self,
        embedding_data: EmbeddingData,
        eps: float = 0.5,
        min_samples: int = 5,
        *,
        embedding_type: Literal["image", "text", "combined"] = "combined",
        standardise: bool = True,
    ) -> ClusteringResults:
        """Perform DBSCAN clustering on weather embeddings.

        Args:
            embedding_data (`EmbeddingData`): Embedding data to cluster.
            eps (`float`): Maximum distance between samples in the same neighbourhood.
            min_samples (`int`): Minimum number of samples in a neighbourhood.
            embedding_type (`str`): Type of embeddings to use ('image', 'text',
                'combined').
            standardise (`bool`): Whether to standardize embeddings before clustering.

        Returns:
            `ClusteringResults`: Container for clustering analysis.
        """
        embeddings = self.prepare_embeddings(embedding_data, embedding_type)

        if standardise:
            scaler = StandardScaler()
            embeddings = scaler.fit_transform(embeddings)

        # Perform DBSCAN clustering
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine")
        labels = dbscan.fit_predict(embeddings)

        return self.create_clustering_results(
            embeddings=embeddings,
            labels=labels,
            cluster_centers=None,
            embedding_data=embedding_data,
        )

    def cluster_hierarchical(
        self,
        embedding_data: EmbeddingData,
        n_clusters: int = 8,
        *,
        embedding_type: Literal["image", "text", "combined"] = "combined",
        linkage_method: str = "ward",
        distance_metric: str = "euclidean",
        standardise: bool = True,
    ) -> ClusteringResults:
        """Perform hierarchical clustering on weather embeddings.

        Args:
            embedding_data (`EmbeddingData`): Embedding data to cluster.
            n_clusters (`int`): Number of clusters to create.
            embedding_type (`str`): Type of embeddings to use ('image', 'text',
                'combined').
            linkage_method (`str): Linkage method ('ward', 'complete', 'average',
                'single').
            distance_metric (`str`): Distance metric for clustering.
            standardise (`bool`): Whether to standardize embeddings before clustering.

        Returns:
            `ClusteringResults`: Container for clustering analysis.
        """
        embeddings = self.prepare_embeddings(embedding_data, embedding_type)

        if standardise:
            scaler = StandardScaler()
            embeddings = scaler.fit_transform(embeddings)

        # Perform hierarchical clustering
        linkage_matrix = linkage(
            embeddings, method=linkage_method, metric=distance_metric
        )
        labels = (
            fcluster(linkage_matrix, n_clusters, criterion="maxclust") - 1
        )  # Convert to 0-based

        return self.create_clustering_results(
            embeddings=embeddings,
            labels=labels,
            cluster_centers=None,
            embedding_data=embedding_data,
        )

    def find_optimal_clusters(
        self,
        embedding_data: EmbeddingData,
        *,
        embedding_type: Literal["image", "text", "combined"] = "combined",
        k_range: tuple[int, int] = (2, 15),
        method: Literal["dbscan", "kmeans"] = "kmeans",
    ) -> dict[str, Any]:
        """Find optimal number of clusters using multiple metrics.

        Args:
            embedding_data (`EmbeddingData`): Embedding data to analyse.
            embedding_type (`str`): Type of embeddings to use.
            k_range (`tuple[int, int]`): Range of cluster numbers to test (min,
                max).
            method (`str`): Clustering method to use ('kmeans' or 'dbscan')

        Returns:
            `dict[str, Any]`: Dictionary containing optimal cluster analysis.
        """
        embeddings = self.prepare_embeddings(embedding_data, embedding_type)
        scaler = StandardScaler()
        embeddings = scaler.fit_transform(embeddings)

        results = {
            "k_values": [],
            "silhouette_scores": [],
            "calinski_harabasz_scores": [],
            "inertias": [],
        }

        k_min, k_max = k_range

        for k in range(k_min, k_max + 1):
            if method == "kmeans":
                clusterer = KMeans(
                    n_clusters=k, random_state=self.random_state, n_init=10
                )
                labels = clusterer.fit_predict(embeddings)
                results["inertias"].append(clusterer.inertia_)
            else:
                msg = f"Optimal cluster finding not implemented for {method}"
                raise NotImplementedError(msg)

            # Calculate quality metrics
            if len(set(labels)) > 1:  # Need at least 2 clusters for metrics
                sil_score = silhouette_score(embeddings, labels)
                ch_score = calinski_harabasz_score(embeddings, labels)
            else:
                sil_score = -1.0
                ch_score = 0.0

            results["k_values"].append(k)
            results["silhouette_scores"].append(sil_score)
            results["calinski_harabasz_scores"].append(ch_score)

        # Find optimal k using different criteria
        sil_scores = np.array(results["silhouette_scores"])
        ch_scores = np.array(results["calinski_harabasz_scores"])

        results["optimal_k"] = {
            "silhouette": k_min + np.argmax(sil_scores),
            "calinski_harabasz": k_min + np.argmax(ch_scores),
        }

        if method == "kmeans":
            # Elbow method using second derivative
            inertias = np.array(results["inertias"])
            if len(inertias) >= 3:  # noqa: PLR2004
                second_deriv = np.diff(inertias, 2)
                elbow_idx = np.argmax(second_deriv) + 1  # +1 for diff offset
                results["optimal_k"]["elbow"] = k_min + elbow_idx

        logger.info(
            "Optimal clusters",
            silhouette=results["optimal_k"]["silhouette"],
            calinski_harabasz=results["optimal_k"]["calinski_harabasz"],
        )

        return results

    def analyse_patterns(
        self, clustering_results: ClusteringResults, embedding_data: EmbeddingData
    ) -> dict[str, Any]:
        """Analyse patterns within clusters.

        Args:
            clustering_results: Results from clustering
            embedding_data: Original embedding data with metadata

        Returns:
            Dictionary containing weather pattern analysis
        """
        labels = clustering_results.labels
        metadata = embedding_data.metadata

        unique_labels = np.unique(labels)
        n_clusters = len(unique_labels[unique_labels >= 0])  # Exclude noise (-1)

        cluster_analysis = {}

        for cluster_id in unique_labels:
            if cluster_id == -1:  # Skip noise cluster in DBSCAN
                continue

            # Get samples in this cluster
            cluster_mask = labels == cluster_id
            cluster_metadata = [metadata[i] for i in np.where(cluster_mask)[0]]

            if not cluster_metadata:
                continue

            # Analyze weather patterns in this cluster
            cluster_analysis[f"cluster_{cluster_id}"] = self.analyse_cluster(
                cluster_metadata
            )

        # Overall analysis
        return {
            "n_clusters": n_clusters,
            "cluster_sizes": {
                f"cluster_{cid}": int(np.sum(labels == cid))
                for cid in unique_labels
                if cid >= 0
            },
            "noise_samples": int(np.sum(labels == -1)) if -1 in labels else 0,
            "weather_patterns": cluster_analysis,
        }

    @staticmethod
    def prepare_embeddings(
        embedding_data: EmbeddingData,
        embedding_type: Literal["image", "text", "combined"] = "combined",
    ) -> np.ndarray:
        """Prepare embeddings for clustering based on type.

        Args:
            embedding_data (`EmbeddingData`): Embedding data to prepare.
            embedding_type (`str`): Type of embeddings to use ('image', 'text',
                'combined').

        Returns:
            `np.ndarray`: Prepared embeddings.

        Raises:
            ValueError: If an unsupported embedding type is specified.
        """
        if embedding_type == "image":
            return embedding_data.image_embeddings.numpy()
        if embedding_type == "text":
            return embedding_data.text_embeddings.numpy()
        if embedding_type == "combined":
            # Concatenate image and text embeddings
            img_emb = embedding_data.image_embeddings.numpy()
            txt_emb = embedding_data.text_embeddings.numpy()
            return np.concatenate([img_emb, txt_emb], axis=1)
        msg = f"Unsupported embedding type: {embedding_type}"
        raise ValueError(msg)

    def create_clustering_results(
        self,
        embeddings: np.ndarray,
        labels: np.ndarray,
        cluster_centers: np.ndarray | None,
        embedding_data: EmbeddingData,
    ) -> ClusteringResults:
        """Create comprehensive clustering results.

        Args:
            embeddings (`np.ndarray`): Original embeddings used for clustering.
            labels (`np.ndarray`): Cluster labels for each sample.
            cluster_centers (`np.ndarray` or `None`): Centers of clusters (if
                applicable).
            embedding_data (`EmbeddingData`): Original embedding data with metadata.

        Returns:
            `ClusteringResults`: Container for clustering analysis.
        """
        # Calculate quality metrics
        unique_labels = np.unique(labels)
        n_clusters = len(unique_labels[unique_labels >= 0])

        if n_clusters > 1:
            sil_score = silhouette_score(embeddings, labels)
            ch_score = calinski_harabasz_score(embeddings, labels)
        else:
            sil_score = -1.0
            ch_score = 0.0

        # Perform dimensionality reduction for visualization
        logger.info(
            "Performing dimensionality reduction using %s", self.reduction_method
        )
        reduced_embeddings = self.reducer.fit_transform(embeddings)

        # Analyze metadata patterns
        metadata_analysis = self.analyse_patterns(
            ClusteringResults(
                labels=labels,
                n_clusters=n_clusters,
                silhouette_score=sil_score,
                calinski_harabasz_score=ch_score,
                reduced_embeddings=reduced_embeddings,
                cluster_centers=cluster_centers,
                metadata_analysis={},
            ),
            embedding_data,
        )

        return ClusteringResults(
            labels=labels,
            n_clusters=n_clusters,
            silhouette_score=sil_score,
            calinski_harabasz_score=ch_score,
            reduced_embeddings=reduced_embeddings,
            cluster_centers=cluster_centers,
            metadata_analysis=metadata_analysis,
        )

    @staticmethod
    def analyse_cluster(cluster_metadata: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyse patterns within a single cluster.

        Args:
            cluster_metadata (`list[dict[str, Any]]`): Metadata for samples in the
                cluster.

        Returns:
            `dict[str, Any]`: Analysis of weather patterns in the cluster.
        """
        # Extract weather attributes
        temperatures = [
            meta.get("avg_temperature")
            for meta in cluster_metadata
            if meta.get("avg_temperature") is not None
        ]
        wind_speeds = [
            meta.get("wind_speed")
            for meta in cluster_metadata
            if meta.get("wind_speed") is not None
        ]
        humidities = [
            meta.get("humidity")
            for meta in cluster_metadata
            if meta.get("humidity") is not None
        ]

        regions = [meta.get("region", "Unknown") for meta in cluster_metadata]
        seasons = [meta.get("season", "Unknown") for meta in cluster_metadata]
        precipitations = [
            meta.get("precipitation", "Unknown") for meta in cluster_metadata
        ]

        return {
            "size": len(cluster_metadata),
            "temperature": {
                "mean": float(np.mean(temperatures)) if temperatures else None,
                "std": float(np.std(temperatures)) if temperatures else None,
                "min": float(np.min(temperatures)) if temperatures else None,
                "max": float(np.max(temperatures)) if temperatures else None,
            },
            "wind_speed": {
                "mean": float(np.mean(wind_speeds)) if wind_speeds else None,
                "std": float(np.std(wind_speeds)) if wind_speeds else None,
            },
            "humidity": {
                "mean": float(np.mean(humidities)) if humidities else None,
                "std": float(np.std(humidities)) if humidities else None,
            },
            "regions": dict(Counter(regions)),
            "seasons": dict(Counter(seasons)),
            "precipitation_types": dict(Counter(precipitations)),
            "dominant_region": Counter(regions).most_common(1)[0][0]
            if regions
            else None,
            "dominant_season": Counter(seasons).most_common(1)[0][0]
            if seasons
            else None,
        }
